{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Kaggle Project**\n",
    "## **Irem Nesli Erez & Ezgi Siir Kibris**\n",
    "\n",
    "### April 10, 2022 with the time extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook preprocesses the training and test sets using mainly the nltk package. [1] Because the data sizes are big and we try various methods for the given classification problem, in order to save time on preparing the data on each file, we preprocess it once and then, continue with that version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-04-04T14:57:09.653499Z",
     "iopub.status.busy": "2022-04-04T14:57:09.652754Z",
     "iopub.status.idle": "2022-04-04T14:57:09.668938Z",
     "shell.execute_reply": "2022-04-04T14:57:09.668035Z",
     "shell.execute_reply.started": "2022-04-04T14:57:09.653380Z"
    }
   },
   "outputs": [],
   "source": [
    "# This section of the code is added by default by Kaggle.\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T15:33:21.468706Z",
     "iopub.status.busy": "2022-04-04T15:33:21.467598Z",
     "iopub.status.idle": "2022-04-04T15:33:21.475794Z",
     "shell.execute_reply": "2022-04-04T15:33:21.474920Z",
     "shell.execute_reply.started": "2022-04-04T15:33:21.468643Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "import nltk #language processing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer #for bag of words\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import interpolate\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "path = '/Users/nesli/Desktop/SPRING2022/DSCC465/Kaggle_istanbul/'\n",
    "os.chdir(path)\n",
    "\n",
    "import random\n",
    "random.seed(465) #seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T14:57:10.394242Z",
     "iopub.status.busy": "2022-04-04T14:57:10.394025Z",
     "iopub.status.idle": "2022-04-04T14:57:12.647660Z",
     "shell.execute_reply": "2022-04-04T14:57:12.646729Z",
     "shell.execute_reply.started": "2022-04-04T14:57:10.394215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592798</th>\n",
       "      <td>3</td>\n",
       "      <td>b'This time, it focused on careers in #publics...</td>\n",
       "      <td>publicservice publicsafety</td>\n",
       "      <td>0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592799</th>\n",
       "      <td>5</td>\n",
       "      <td>b'.#StormyDaniels, #MichaelWolfe, #JamesComey ...</td>\n",
       "      <td>StormyDaniels MichaelWolfe JamesComey</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592800</th>\n",
       "      <td>33</td>\n",
       "      <td>b'@NRDems The American people deserve the trut...</td>\n",
       "      <td>CultureOfCorruption</td>\n",
       "      <td>14</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592801</th>\n",
       "      <td>4</td>\n",
       "      <td>b'Only 2 weeks left to submit your #app to the...</td>\n",
       "      <td>app copolitics CAC16 HouseOfCode co06</td>\n",
       "      <td>3</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592802</th>\n",
       "      <td>155</td>\n",
       "      <td>b'The #MuslimBan remains as un-American and of...</td>\n",
       "      <td>MuslimBan</td>\n",
       "      <td>48</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>592803 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        favorite_count                                          full_text  \\\n",
       "0                    0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1                  258  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2                    0  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3                    9  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4                    3  b'Pleased to support @amergateways at their Ju...   \n",
       "...                ...                                                ...   \n",
       "592798               3  b'This time, it focused on careers in #publics...   \n",
       "592799               5  b'.#StormyDaniels, #MichaelWolfe, #JamesComey ...   \n",
       "592800              33  b'@NRDems The American people deserve the trut...   \n",
       "592801               4  b'Only 2 weeks left to submit your #app to the...   \n",
       "592802             155  b'The #MuslimBan remains as un-American and of...   \n",
       "\n",
       "                                     hashtags  retweet_count    year party_id  \n",
       "0                                        KUSI             10  2017.0        R  \n",
       "1                                 Coronavirus            111  2020.0        R  \n",
       "2                                        MO03              2  2014.0        R  \n",
       "3                        TeamUSA WorldJuniors              3  2017.0        R  \n",
       "4                      ImmigrantHeritageMonth              3  2019.0        D  \n",
       "...                                       ...            ...     ...      ...  \n",
       "592798             publicservice publicsafety              0  2017.0        R  \n",
       "592799  StormyDaniels MichaelWolfe JamesComey              1  2018.0        R  \n",
       "592800                    CultureOfCorruption             14  2020.0        D  \n",
       "592801  app copolitics CAC16 HouseOfCode co06              3  2016.0        R  \n",
       "592802                              MuslimBan             48  2020.0        D  \n",
       "\n",
       "[592803 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data set\n",
    "data_train = pd.read_csv('congressional_tweet_training_data.csv')\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T14:57:12.648963Z",
     "iopub.status.busy": "2022-04-04T14:57:12.648732Z",
     "iopub.status.idle": "2022-04-04T14:57:13.717922Z",
     "shell.execute_reply": "2022-04-04T14:57:13.716889Z",
     "shell.execute_reply.started": "2022-04-04T14:57:12.648934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>b'#TaxReform improved the playing field for Am...</td>\n",
       "      <td>TaxReform</td>\n",
       "      <td>13</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>b'This #NativeWomensEqualPay Day, we recommit ...</td>\n",
       "      <td>NativeWomensEqualPay</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>b\"\\xe2\\x80\\x9cI became convinced that our gene...</td>\n",
       "      <td>MeToo ShatteringTheSilence</td>\n",
       "      <td>24</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>b'During #NationalAdoptionMonth, we honor the ...</td>\n",
       "      <td>NationalAdoptionMonth</td>\n",
       "      <td>2</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>b'Happy #AirborneDay to our @USArmy paratroope...</td>\n",
       "      <td>AirborneDay AirborneAllTheWay</td>\n",
       "      <td>7</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264995</th>\n",
       "      <td>264995</td>\n",
       "      <td>516</td>\n",
       "      <td>b'We need to #ExtendCHIP before a single child...</td>\n",
       "      <td>ExtendCHIP</td>\n",
       "      <td>223</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264996</th>\n",
       "      <td>264996</td>\n",
       "      <td>0</td>\n",
       "      <td>b\"Our #ObamaCare investigation continued today...</td>\n",
       "      <td>ObamaCare</td>\n",
       "      <td>3</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264997</th>\n",
       "      <td>264997</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Congratulations to the new #MissTeenUSA Loga...</td>\n",
       "      <td>MissTeenUSA CT</td>\n",
       "      <td>4</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264998</th>\n",
       "      <td>264998</td>\n",
       "      <td>2174</td>\n",
       "      <td>b'Speaking of dishonesty. Nothing like being c...</td>\n",
       "      <td>mosen</td>\n",
       "      <td>1168</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264999</th>\n",
       "      <td>264999</td>\n",
       "      <td>6</td>\n",
       "      <td>b'In honor of #ConstitutionDay, we remember th...</td>\n",
       "      <td>ConstitutionDay</td>\n",
       "      <td>3</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  favorite_count  \\\n",
       "0            0              70   \n",
       "1            1              27   \n",
       "2            2              49   \n",
       "3            3              14   \n",
       "4            4              13   \n",
       "...        ...             ...   \n",
       "264995  264995             516   \n",
       "264996  264996               0   \n",
       "264997  264997               1   \n",
       "264998  264998            2174   \n",
       "264999  264999               6   \n",
       "\n",
       "                                                full_text  \\\n",
       "0       b'#TaxReform improved the playing field for Am...   \n",
       "1       b'This #NativeWomensEqualPay Day, we recommit ...   \n",
       "2       b\"\\xe2\\x80\\x9cI became convinced that our gene...   \n",
       "3       b'During #NationalAdoptionMonth, we honor the ...   \n",
       "4       b'Happy #AirborneDay to our @USArmy paratroope...   \n",
       "...                                                   ...   \n",
       "264995  b'We need to #ExtendCHIP before a single child...   \n",
       "264996  b\"Our #ObamaCare investigation continued today...   \n",
       "264997  b'Congratulations to the new #MissTeenUSA Loga...   \n",
       "264998  b'Speaking of dishonesty. Nothing like being c...   \n",
       "264999  b'In honor of #ConstitutionDay, we remember th...   \n",
       "\n",
       "                             hashtags  retweet_count    year party  \n",
       "0                           TaxReform             13  2018.0     D  \n",
       "1                NativeWomensEqualPay             11     NaN     D  \n",
       "2          MeToo ShatteringTheSilence             24  2017.0     D  \n",
       "3               NationalAdoptionMonth              2  2019.0     D  \n",
       "4       AirborneDay AirborneAllTheWay              7  2018.0     D  \n",
       "...                               ...            ...     ...   ...  \n",
       "264995                     ExtendCHIP            223  2017.0     D  \n",
       "264996                      ObamaCare              3  2013.0     D  \n",
       "264997                 MissTeenUSA CT              4  2012.0     D  \n",
       "264998                          mosen           1168  2018.0     D  \n",
       "264999                ConstitutionDay              3  2015.0     D  \n",
       "\n",
       "[265000 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data set\n",
    "data_test = pd.read_csv('congressional_tweet_test_data.csv')\n",
    "data_test #party is set to D by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18712"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for number of rows that have NaN [2]\n",
    "data_train.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03156529234838555"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().any(axis=1).sum()/len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8347"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for number of rows that have NaN [2]\n",
    "data_test.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03149811320754717"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.isnull().any(axis=1).sum()/len(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is only $3\\%$, we can drop the rows with \"NaN\" values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=data_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the indexing after dropping some rows [3]\n",
    "data_train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix the indexing after dropping some rows [3]\n",
    "data_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574086</th>\n",
       "      <td>3</td>\n",
       "      <td>b'This time, it focused on careers in #publics...</td>\n",
       "      <td>publicservice publicsafety</td>\n",
       "      <td>0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574087</th>\n",
       "      <td>5</td>\n",
       "      <td>b'.#StormyDaniels, #MichaelWolfe, #JamesComey ...</td>\n",
       "      <td>StormyDaniels MichaelWolfe JamesComey</td>\n",
       "      <td>1</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574088</th>\n",
       "      <td>33</td>\n",
       "      <td>b'@NRDems The American people deserve the trut...</td>\n",
       "      <td>CultureOfCorruption</td>\n",
       "      <td>14</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574089</th>\n",
       "      <td>4</td>\n",
       "      <td>b'Only 2 weeks left to submit your #app to the...</td>\n",
       "      <td>app copolitics CAC16 HouseOfCode co06</td>\n",
       "      <td>3</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574090</th>\n",
       "      <td>155</td>\n",
       "      <td>b'The #MuslimBan remains as un-American and of...</td>\n",
       "      <td>MuslimBan</td>\n",
       "      <td>48</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574091 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        favorite_count                                          full_text  \\\n",
       "0                    0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1                  258  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2                    0  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3                    9  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4                    3  b'Pleased to support @amergateways at their Ju...   \n",
       "...                ...                                                ...   \n",
       "574086               3  b'This time, it focused on careers in #publics...   \n",
       "574087               5  b'.#StormyDaniels, #MichaelWolfe, #JamesComey ...   \n",
       "574088              33  b'@NRDems The American people deserve the trut...   \n",
       "574089               4  b'Only 2 weeks left to submit your #app to the...   \n",
       "574090             155  b'The #MuslimBan remains as un-American and of...   \n",
       "\n",
       "                                     hashtags  retweet_count    year party_id  \n",
       "0                                        KUSI             10  2017.0        R  \n",
       "1                                 Coronavirus            111  2020.0        R  \n",
       "2                                        MO03              2  2014.0        R  \n",
       "3                        TeamUSA WorldJuniors              3  2017.0        R  \n",
       "4                      ImmigrantHeritageMonth              3  2019.0        D  \n",
       "...                                       ...            ...     ...      ...  \n",
       "574086             publicservice publicsafety              0  2017.0        R  \n",
       "574087  StormyDaniels MichaelWolfe JamesComey              1  2018.0        R  \n",
       "574088                    CultureOfCorruption             14  2020.0        D  \n",
       "574089  app copolitics CAC16 HouseOfCode co06              3  2016.0        R  \n",
       "574090                              MuslimBan             48  2020.0        D  \n",
       "\n",
       "[574091 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>b'#TaxReform improved the playing field for Am...</td>\n",
       "      <td>TaxReform</td>\n",
       "      <td>13</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>b\"\\xe2\\x80\\x9cI became convinced that our gene...</td>\n",
       "      <td>MeToo ShatteringTheSilence</td>\n",
       "      <td>24</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>b'During #NationalAdoptionMonth, we honor the ...</td>\n",
       "      <td>NationalAdoptionMonth</td>\n",
       "      <td>2</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>b'Happy #AirborneDay to our @USArmy paratroope...</td>\n",
       "      <td>AirborneDay AirborneAllTheWay</td>\n",
       "      <td>7</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>b\"Proud to join 200+ colleagues in SCOTUS amic...</td>\n",
       "      <td>LGBT</td>\n",
       "      <td>13</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256648</th>\n",
       "      <td>264995</td>\n",
       "      <td>516</td>\n",
       "      <td>b'We need to #ExtendCHIP before a single child...</td>\n",
       "      <td>ExtendCHIP</td>\n",
       "      <td>223</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256649</th>\n",
       "      <td>264996</td>\n",
       "      <td>0</td>\n",
       "      <td>b\"Our #ObamaCare investigation continued today...</td>\n",
       "      <td>ObamaCare</td>\n",
       "      <td>3</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256650</th>\n",
       "      <td>264997</td>\n",
       "      <td>1</td>\n",
       "      <td>b'Congratulations to the new #MissTeenUSA Loga...</td>\n",
       "      <td>MissTeenUSA CT</td>\n",
       "      <td>4</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256651</th>\n",
       "      <td>264998</td>\n",
       "      <td>2174</td>\n",
       "      <td>b'Speaking of dishonesty. Nothing like being c...</td>\n",
       "      <td>mosen</td>\n",
       "      <td>1168</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256652</th>\n",
       "      <td>264999</td>\n",
       "      <td>6</td>\n",
       "      <td>b'In honor of #ConstitutionDay, we remember th...</td>\n",
       "      <td>ConstitutionDay</td>\n",
       "      <td>3</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256653 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id  favorite_count  \\\n",
       "0            0              70   \n",
       "1            2              49   \n",
       "2            3              14   \n",
       "3            4              13   \n",
       "4            5              73   \n",
       "...        ...             ...   \n",
       "256648  264995             516   \n",
       "256649  264996               0   \n",
       "256650  264997               1   \n",
       "256651  264998            2174   \n",
       "256652  264999               6   \n",
       "\n",
       "                                                full_text  \\\n",
       "0       b'#TaxReform improved the playing field for Am...   \n",
       "1       b\"\\xe2\\x80\\x9cI became convinced that our gene...   \n",
       "2       b'During #NationalAdoptionMonth, we honor the ...   \n",
       "3       b'Happy #AirborneDay to our @USArmy paratroope...   \n",
       "4       b\"Proud to join 200+ colleagues in SCOTUS amic...   \n",
       "...                                                   ...   \n",
       "256648  b'We need to #ExtendCHIP before a single child...   \n",
       "256649  b\"Our #ObamaCare investigation continued today...   \n",
       "256650  b'Congratulations to the new #MissTeenUSA Loga...   \n",
       "256651  b'Speaking of dishonesty. Nothing like being c...   \n",
       "256652  b'In honor of #ConstitutionDay, we remember th...   \n",
       "\n",
       "                             hashtags  retweet_count    year party  \n",
       "0                           TaxReform             13  2018.0     D  \n",
       "1          MeToo ShatteringTheSilence             24  2017.0     D  \n",
       "2               NationalAdoptionMonth              2  2019.0     D  \n",
       "3       AirborneDay AirborneAllTheWay              7  2018.0     D  \n",
       "4                                LGBT             13  2017.0     D  \n",
       "...                               ...            ...     ...   ...  \n",
       "256648                     ExtendCHIP            223  2017.0     D  \n",
       "256649                      ObamaCare              3  2013.0     D  \n",
       "256650                 MissTeenUSA CT              4  2012.0     D  \n",
       "256651                          mosen           1168  2018.0     D  \n",
       "256652                ConstitutionDay              3  2015.0     D  \n",
       "\n",
       "[256653 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kodu hizlandirmak icin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_train[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_train=data_train[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_test=data_test[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/96l3vfgd54s0dlvms03s3z400000gn/T/ipykernel_4377/552341189.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train['party_class']=data_train['party_id'].apply(lambda x: 0 if x=='D' else 1)\n"
     ]
    }
   ],
   "source": [
    "# Make the classes numerical\n",
    "\n",
    "data_train['party_class']=data_train['party_id'].apply(lambda x: 0 if x=='D' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/96l3vfgd54s0dlvms03s3z400000gn/T/ipykernel_4377/1761763176.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_test['party_class']=data_test['party'].apply(lambda x: 0 if x=='D' else 1)\n"
     ]
    }
   ],
   "source": [
    "data_test['party_class']=data_test['party'].apply(lambda x: 0 if x=='D' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the full_text column of the data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T14:57:13.720090Z",
     "iopub.status.busy": "2022-04-04T14:57:13.719750Z",
     "iopub.status.idle": "2022-04-04T14:57:13.725507Z",
     "shell.execute_reply": "2022-04-04T14:57:13.724553Z",
     "shell.execute_reply.started": "2022-04-04T14:57:13.720056Z"
    }
   },
   "source": [
    "For the following parts of the text preprocessing, we follow from our homework 4 codes [4], [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T14:57:13.971483Z",
     "iopub.status.busy": "2022-04-04T14:57:13.971241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokens_list=[word_tokenize(sent) for sent in data_train['full_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2022-04-04T15:20:23.201600Z",
     "shell.execute_reply": "2022-04-04T15:20:23.200668Z",
     "shell.execute_reply.started": "2022-04-04T15:02:47.703855Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pos Tag \n",
    "tagged_tokens =[pos_tag(sent) for sent in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T15:20:23.202919Z",
     "iopub.status.busy": "2022-04-04T15:20:23.202696Z",
     "iopub.status.idle": "2022-04-04T15:20:23.208896Z",
     "shell.execute_reply": "2022-04-04T15:20:23.208015Z",
     "shell.execute_reply.started": "2022-04-04T15:20:23.202891Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag): #Following from [6]\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None # for easy if-statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T15:20:23.211832Z",
     "iopub.status.busy": "2022-04-04T15:20:23.211595Z",
     "iopub.status.idle": "2022-04-04T15:20:23.225772Z",
     "shell.execute_reply": "2022-04-04T15:20:23.224761Z",
     "shell.execute_reply.started": "2022-04-04T15:20:23.211802Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(data_train['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['tagged_tokens']=tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_list = x['tagged_tokens'].transform(lambda value: ' '.join([lemmatizer.lemmatize(a[0],pos=get_wordnet_pos(a[1])) if get_wordnet_pos(a[1]) else a[0] for a in  value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['lemma_list']=lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>tagged_tokens</th>\n",
       "      <th>lemma_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>[(b, NN), ('', ''), (RT, NNP), (@, NNP), (KUSI...</td>\n",
       "      <td>b '' RT @ KUSINews : One of our longtime viewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>[(b, NN), ('', ''), (Today, NN), (I, PRP), ('m...</td>\n",
       "      <td>b '' Today I 'm urge the @ CDCgov to immediate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>[(b'Tomorrow, NN), (,, ,), (#, #), (MO03, NNP)...</td>\n",
       "      <td>b'Tomorrow , # MO03 senior graduate from Calva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>[(b'Congrats, NNS), (to, TO), (#, #), (TeamUSA...</td>\n",
       "      <td>b'Congrats to # TeamUSA and Canton Native @ JG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>[(b'Pleased, VBN), (to, TO), (support, VB), (@...</td>\n",
       "      <td>b'Pleased to support @ amergateways at their J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text  \\\n",
       "0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4  b'Pleased to support @amergateways at their Ju...   \n",
       "\n",
       "                                       tagged_tokens  \\\n",
       "0  [(b, NN), ('', ''), (RT, NNP), (@, NNP), (KUSI...   \n",
       "1  [(b, NN), ('', ''), (Today, NN), (I, PRP), ('m...   \n",
       "2  [(b'Tomorrow, NN), (,, ,), (#, #), (MO03, NNP)...   \n",
       "3  [(b'Congrats, NNS), (to, TO), (#, #), (TeamUSA...   \n",
       "4  [(b'Pleased, VBN), (to, TO), (support, VB), (@...   \n",
       "\n",
       "                                          lemma_list  \n",
       "0  b '' RT @ KUSINews : One of our longtime viewe...  \n",
       "1  b '' Today I 'm urge the @ CDCgov to immediate...  \n",
       "2  b'Tomorrow , # MO03 senior graduate from Calva...  \n",
       "3  b'Congrats to # TeamUSA and Canton Native @ JG...  \n",
       "4  b'Pleased to support @ amergateways at their J...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers, words, characters, punctuation, links and emojis \n",
    "\n",
    "def emoji_free_text(text): # From [7] \n",
    "    return emoji.get_emoji_regexp().sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        lower_token = token.lower()\n",
    "        len_check = len(lower_token) >= 2\n",
    "        #start_check = not (lower_token.startswith('http') or lower_token.startswith('\\\\') or lower_token.startswith(':'))\n",
    "        stop_word_check = lower_token not in stop_words\n",
    "        if len_check and stop_word_check:\n",
    "            lower_token=re.sub(r'\\d+', '', lower_token) \n",
    "            lower_token=re.sub(r'\\b\\w{1}\\b', '',  lower_token)  \n",
    "            lower_token=re.sub(r'[^\\w\\s]', '', lower_token)                # Remove punctuation\n",
    "            lower_token=re.sub(r'http\\S+', '',lower_token)                 # Remove links\n",
    "            cleaned_tokens.append(re.sub('[,.!?]|<br \\/>\\+|<br \\/>', '', lower_token))\n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/96l3vfgd54s0dlvms03s3z400000gn/T/ipykernel_4377/3666141621.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_train['text_clean']=Text_cleaner\n"
     ]
    }
   ],
   "source": [
    "Text_clean = x['lemma_list'].map(clean_text)\n",
    "Text_cleaner = Text_clean .map(emoji_free_text)\n",
    "data_train['text_clean']=Text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party_id</th>\n",
       "      <th>party_class</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>rt kusinews one longtime viewer congressman d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>today  urge cdcgov immediately launch  phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>tomorrow mo senior graduate calvary lutheran f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>congrats teamusa canton native jgreenway win w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>pleased support amergateways june fiesta honor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   favorite_count                                          full_text  \\\n",
       "0               0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1             258  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2               0  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3               9  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4               3  b'Pleased to support @amergateways at their Ju...   \n",
       "\n",
       "                 hashtags  retweet_count    year party_id  party_class  \\\n",
       "0                    KUSI             10  2017.0        R            1   \n",
       "1             Coronavirus            111  2020.0        R            1   \n",
       "2                    MO03              2  2014.0        R            1   \n",
       "3    TeamUSA WorldJuniors              3  2017.0        R            1   \n",
       "4  ImmigrantHeritageMonth              3  2019.0        D            0   \n",
       "\n",
       "                                          text_clean  \n",
       "0   rt kusinews one longtime viewer congressman d...  \n",
       "1   today  urge cdcgov immediately launch  phone ...  \n",
       "2  tomorrow mo senior graduate calvary lutheran f...  \n",
       "3  congrats teamusa canton native jgreenway win w...  \n",
       "4  pleased support amergateways june fiesta honor...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, repeat the cleaning for the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokens_list=[word_tokenize(sent) for sent in data_test['full_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos Tag \n",
    "tagged_tokens =[pos_tag(sent) for sent in tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag): #Following from [6]\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None # for easy if-statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(data_test['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['tagged_tokens']=tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_list = x['tagged_tokens'].transform(lambda value: ' '.join([lemmatizer.lemmatize(a[0],pos=get_wordnet_pos(a[1])) if get_wordnet_pos(a[1]) else a[0] for a in  value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['lemma_list']=lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/96l3vfgd54s0dlvms03s3z400000gn/T/ipykernel_4377/1101272201.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_test['text_clean']=Text_cleaner\n"
     ]
    }
   ],
   "source": [
    "Text_clean = x['lemma_list'].map(clean_text)\n",
    "Text_cleaner = Text_clean .map(emoji_free_text)\n",
    "data_test['text_clean']=Text_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party</th>\n",
       "      <th>party_class</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>b'#TaxReform improved the playing field for Am...</td>\n",
       "      <td>TaxReform</td>\n",
       "      <td>13</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>taxreform improve playing field american worke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>b\"\\xe2\\x80\\x9cI became convinced that our gene...</td>\n",
       "      <td>MeToo ShatteringTheSilence</td>\n",
       "      <td>24</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>xexci become convinced generation silence mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>b'During #NationalAdoptionMonth, we honor the ...</td>\n",
       "      <td>NationalAdoptionMonth</td>\n",
       "      <td>2</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>during nationaladoptionmonth honor adoptive pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>b'Happy #AirborneDay to our @USArmy paratroope...</td>\n",
       "      <td>AirborneDay AirborneAllTheWay</td>\n",
       "      <td>7</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>happy airborneday usarmy paratrooper veteran s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>b\"Proud to join 200+ colleagues in SCOTUS amic...</td>\n",
       "      <td>LGBT</td>\n",
       "      <td>13</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>proud join  colleague scotus amicus brief arg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  favorite_count                                          full_text  \\\n",
       "0   0              70  b'#TaxReform improved the playing field for Am...   \n",
       "1   2              49  b\"\\xe2\\x80\\x9cI became convinced that our gene...   \n",
       "2   3              14  b'During #NationalAdoptionMonth, we honor the ...   \n",
       "3   4              13  b'Happy #AirborneDay to our @USArmy paratroope...   \n",
       "4   5              73  b\"Proud to join 200+ colleagues in SCOTUS amic...   \n",
       "\n",
       "                        hashtags  retweet_count    year party  party_class  \\\n",
       "0                      TaxReform             13  2018.0     D            0   \n",
       "1     MeToo ShatteringTheSilence             24  2017.0     D            0   \n",
       "2          NationalAdoptionMonth              2  2019.0     D            0   \n",
       "3  AirborneDay AirborneAllTheWay              7  2018.0     D            0   \n",
       "4                           LGBT             13  2017.0     D            0   \n",
       "\n",
       "                                          text_clean  \n",
       "0  taxreform improve playing field american worke...  \n",
       "1   xexci become convinced generation silence mak...  \n",
       "2  during nationaladoptionmonth honor adoptive pa...  \n",
       "3  happy airborneday usarmy paratrooper veteran s...  \n",
       "4   proud join  colleague scotus amicus brief arg...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save some memory space\n",
    "del x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the data\n",
    "\n",
    "Now for vectorization, we can use different techniques as presented in the lecture notes. [8] Try two different methods:\n",
    "\n",
    "1. Bag of words\n",
    "2. Word2vec\n",
    "\n",
    "We will not try TF-IDF because it is rather beneficial for long text data.\n",
    "\n",
    "Considering the fact that the bag of words method includes each unique word in the text as a dimension, it is better to apply vectorization on the merged data set that contains both the test and the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([data_train,data_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>year</th>\n",
       "      <th>party_id</th>\n",
       "      <th>party_class</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Id</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"RT @KUSINews: One of our longtime viewers wa...</td>\n",
       "      <td>KUSI</td>\n",
       "      <td>10</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>rt kusinews one longtime viewer congressman d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>258</td>\n",
       "      <td>b\"Today I'm urging the @CDCgov to immediately ...</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>111</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>today  urge cdcgov immediately launch  phone ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Tomorrow, #MO03 seniors graduate from Calvar...</td>\n",
       "      <td>MO03</td>\n",
       "      <td>2</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>tomorrow mo senior graduate calvary lutheran f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>b'Congrats to #TeamUSA and Canton Native @JGre...</td>\n",
       "      <td>TeamUSA WorldJuniors</td>\n",
       "      <td>3</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>congrats teamusa canton native jgreenway win w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>b'Pleased to support @amergateways at their Ju...</td>\n",
       "      <td>ImmigrantHeritageMonth</td>\n",
       "      <td>3</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>pleased support amergateways june fiesta honor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830739</th>\n",
       "      <td>516</td>\n",
       "      <td>b'We need to #ExtendCHIP before a single child...</td>\n",
       "      <td>ExtendCHIP</td>\n",
       "      <td>223</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>we need extendchip single child lose health co...</td>\n",
       "      <td>264995.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830740</th>\n",
       "      <td>0</td>\n",
       "      <td>b\"Our #ObamaCare investigation continued today...</td>\n",
       "      <td>ObamaCare</td>\n",
       "      <td>3</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>obamacare investigation continue today demand...</td>\n",
       "      <td>264996.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830741</th>\n",
       "      <td>1</td>\n",
       "      <td>b'Congratulations to the new #MissTeenUSA Loga...</td>\n",
       "      <td>MissTeenUSA CT</td>\n",
       "      <td>4</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>congratulations new missteenusa logan west sou...</td>\n",
       "      <td>264997.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830742</th>\n",
       "      <td>2174</td>\n",
       "      <td>b'Speaking of dishonesty. Nothing like being c...</td>\n",
       "      <td>mosen</td>\n",
       "      <td>1168</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>speaking dishonesty nothing like catch camera ...</td>\n",
       "      <td>264998.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830743</th>\n",
       "      <td>6</td>\n",
       "      <td>b'In honor of #ConstitutionDay, we remember th...</td>\n",
       "      <td>ConstitutionDay</td>\n",
       "      <td>3</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>in honor constitutionday remember foundation c...</td>\n",
       "      <td>264999.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>830744 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        favorite_count                                          full_text  \\\n",
       "0                    0  b\"RT @KUSINews: One of our longtime viewers wa...   \n",
       "1                  258  b\"Today I'm urging the @CDCgov to immediately ...   \n",
       "2                    0  b'Tomorrow, #MO03 seniors graduate from Calvar...   \n",
       "3                    9  b'Congrats to #TeamUSA and Canton Native @JGre...   \n",
       "4                    3  b'Pleased to support @amergateways at their Ju...   \n",
       "...                ...                                                ...   \n",
       "830739             516  b'We need to #ExtendCHIP before a single child...   \n",
       "830740               0  b\"Our #ObamaCare investigation continued today...   \n",
       "830741               1  b'Congratulations to the new #MissTeenUSA Loga...   \n",
       "830742            2174  b'Speaking of dishonesty. Nothing like being c...   \n",
       "830743               6  b'In honor of #ConstitutionDay, we remember th...   \n",
       "\n",
       "                      hashtags  retweet_count    year party_id  party_class  \\\n",
       "0                         KUSI             10  2017.0        R            1   \n",
       "1                  Coronavirus            111  2020.0        R            1   \n",
       "2                         MO03              2  2014.0        R            1   \n",
       "3         TeamUSA WorldJuniors              3  2017.0        R            1   \n",
       "4       ImmigrantHeritageMonth              3  2019.0        D            0   \n",
       "...                        ...            ...     ...      ...          ...   \n",
       "830739              ExtendCHIP            223  2017.0      NaN            0   \n",
       "830740               ObamaCare              3  2013.0      NaN            0   \n",
       "830741          MissTeenUSA CT              4  2012.0      NaN            0   \n",
       "830742                   mosen           1168  2018.0      NaN            0   \n",
       "830743         ConstitutionDay              3  2015.0      NaN            0   \n",
       "\n",
       "                                               text_clean        Id party  \n",
       "0        rt kusinews one longtime viewer congressman d...       NaN   NaN  \n",
       "1        today  urge cdcgov immediately launch  phone ...       NaN   NaN  \n",
       "2       tomorrow mo senior graduate calvary lutheran f...       NaN   NaN  \n",
       "3       congrats teamusa canton native jgreenway win w...       NaN   NaN  \n",
       "4       pleased support amergateways june fiesta honor...       NaN   NaN  \n",
       "...                                                   ...       ...   ...  \n",
       "830739  we need extendchip single child lose health co...  264995.0     D  \n",
       "830740   obamacare investigation continue today demand...  264996.0     D  \n",
       "830741  congratulations new missteenusa logan west sou...  264997.0     D  \n",
       "830742  speaking dishonesty nothing like catch camera ...  264998.0     D  \n",
       "830743  in honor constitutionday remember foundation c...  264999.0     D  \n",
       "\n",
       "[830744 rows x 10 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "# Following from [9]\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "text_bow = vectorizer.fit_transform(data['text_clean'])\n",
    "a = np.zeros((830744, 963153), dtype='uint8')\n",
    "a=text_bow.toarray()\n",
    "text_bow_df = pd.DataFrame(a,columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words for hashtags\n",
    "# Following from [9]\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "hash_bow = vectorizer.fit_transform(data['hashtags'])\n",
    "hash_bow_df = pd.DataFrame(hash_bow.toarray(),columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data=len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unnecessary variables\n",
    "\n",
    "del data\n",
    "del text_bow\n",
    "del hash_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply dimensionality reduction to bow_vector and hash_vector to decrease the dimensions to 3 principal components\n",
    "\n",
    "We apply dimensionality reduction to both test and training data sets simultaneously following from our codes for HW6. [10], [11]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following from [12] to use sklearn's PCA \n",
    "\n",
    "#Standardize before calling PCA\n",
    "scaler=preprocessing.StandardScaler().fit(text_bow_df)\n",
    "text_bow_df_scaled=scaler.transform(text_bow_df)\n",
    "\n",
    "sklearn_pca = PCA(n_components=3)\n",
    "pcs = sklearn_pca.fit_transform(text_bow_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pcs to the original data\n",
    "data_train['bow_pcs']=pcs[0:len(data_train)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['bow_pcs']=pcs[len(data_train):len_data].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pc1=np.zeros(len(data_train))\n",
    "bow_pc2=np.zeros(len(data_train))\n",
    "bow_pc3=np.zeros(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the principal component returned\n",
    "for i in np.arange(len(data_train)):\n",
    "    bow_pc1[i]=data_train['bow_pcs'][i][0]\n",
    "    bow_pc2[i]=data_train['bow_pcs'][i][1]\n",
    "    bow_pc3[i]=data_train['bow_pcs'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['bow_pc1']=bow_pc1\n",
    "data_train['bow_pc2']=bow_pc2\n",
    "data_train['bow_pc3']=bow_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add these columns to the test data\n",
    "bow_pc1=np.zeros(len(data_test))\n",
    "bow_pc2=np.zeros(len(data_test))\n",
    "bow_pc3=np.zeros(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_test)):\n",
    "    bow_pc1[i]=data_test['bow_pcs'][i][0]\n",
    "    bow_pc2[i]=data_test['bow_pcs'][i][1]\n",
    "    bow_pc3[i]=data_test['bow_pcs'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['bow_pc1']=bow_pc1\n",
    "data_test['bow_pc2']=bow_pc2\n",
    "data_test['bow_pc3']=bow_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization_code.py\n",
    "# Directly copy-pasted from the provided file for earlier homework [13]\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "df = data_train\n",
    "pal = sns.color_palette(\"Paired\")[:len(set(data_train['party_class']))]\n",
    "p1 = sns.scatterplot(x=\"bow_pc1\", y='bow_pc2', hue='party_class', palette = pal, data=df, s=250, alpha=0.7, legend=False)\n",
    "\n",
    "#For each point, we add a text inside the bubble\n",
    "for line in range(0,df.shape[0]):\n",
    "     p1.text(df.bow_pc1[line], df.bow_pc2[line], df.party_class[line], horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "\n",
    "plt.suptitle('Two-Dimensional Map (PCA)', fontsize=36)\n",
    "plt.xlabel('Dimension 1', fontsize=24)\n",
    "plt.ylabel('Dimension 2', fontsize=24)\n",
    "\n",
    "\n",
    "for i in df.party_class.unique():\n",
    "    # get the convex hull\n",
    "    points = df[df.party_class == i][['bow_pc1', 'bow_pc2']].values\n",
    "    hull = ConvexHull(points)\n",
    "    x_hull = np.append(points[hull.vertices,0],\n",
    "                       points[hull.vertices,0][0])\n",
    "    y_hull = np.append(points[hull.vertices,1],\n",
    "                       points[hull.vertices,1][0])\n",
    "    \n",
    "    # interpolate\n",
    "    dist = np.sqrt((x_hull[:-1] - x_hull[1:])**2 + (y_hull[:-1] - y_hull[1:])**2)\n",
    "    dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    spline, u = interpolate.splprep([x_hull, y_hull], \n",
    "                                    u=dist_along, s=0)\n",
    "    interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    interp_x, interp_y = interpolate.splev(interp_d, spline)\n",
    "    # plot shape\n",
    "    plt.fill(interp_x, interp_y, '--', c=pal[i], alpha=0.2)\n",
    "    \n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral-embedding\n",
    "\n",
    "Again, following from HW6 [10], [11]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following from sklearn's guides [14]\n",
    "\n",
    "embedding = SpectralEmbedding(n_components=3)\n",
    "pcs_embedded = embedding.fit_transform(text_bow_df_scaled) #Use the scaled version of bow_vec column for all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['bow_spec']=pcs_embedded[0:len(data_train)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['bow_spec']=pcs_embedded[len(data_train):len_data].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pc1=np.zeros(len(data_train))\n",
    "bow_pc2=np.zeros(len(data_train))\n",
    "bow_pc3=np.zeros(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_train)):\n",
    "    bow_pc1[i]=data_train['bow_spec'][i][0]\n",
    "    bow_pc2[i]=data_train['bow_spec'][i][1]\n",
    "    bow_pc3[i]=data_train['bow_spec'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['bow_spec1']=bow_pc1\n",
    "data_train['bow_spec2']=bow_pc2\n",
    "data_train['bow_spec3']=bow_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add these columns to the test data\n",
    "\n",
    "bow_pc1=np.zeros(len(data_test))\n",
    "bow_pc2=np.zeros(len(data_test))\n",
    "bow_pc3=np.zeros(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_test)):\n",
    "    bow_pc1[i]=data_test['bow_spec'][i][0]\n",
    "    bow_pc2[i]=data_test['bow_spec'][i][1]\n",
    "    bow_pc3[i]=data_test['bow_spec'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['bow_spec1']=bow_pc1\n",
    "data_test['bow_spec2']=bow_pc2\n",
    "data_test['bow_spec3']=bow_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization_code.py\n",
    "# Directly copy-pasted from the provided file\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "df = data_train\n",
    "pal = sns.color_palette(\"Paired\")[:len(set(data_train['party_class']))]\n",
    "p1 = sns.scatterplot(x=\"bow_spec1\", y='bow_spec2', hue='party_class', palette = pal, data=df, s=250, alpha=0.7, legend=False)\n",
    "\n",
    "#For each point, we add a text inside the bubble\n",
    "for line in range(0,df.shape[0]):\n",
    "     p1.text(df.bow_spec1[line], df.bow_spec2[line], df.party_class[line], horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "\n",
    "plt.suptitle('Two-Dimensional Map (Spectral Embedding)', fontsize=36)\n",
    "plt.xlabel('Dimension 1', fontsize=24)\n",
    "plt.ylabel('Dimension 2', fontsize=24)\n",
    "\n",
    "\n",
    "for i in df.party_class.unique():\n",
    "    # get the convex hull\n",
    "    points = df[df.party_class == i][['bow_spec1', 'bow_spec2']].values\n",
    "    hull = ConvexHull(points)\n",
    "    x_hull = np.append(points[hull.vertices,0],\n",
    "                       points[hull.vertices,0][0])\n",
    "    y_hull = np.append(points[hull.vertices,1],\n",
    "                       points[hull.vertices,1][0])\n",
    "    \n",
    "    # interpolate\n",
    "    dist = np.sqrt((x_hull[:-1] - x_hull[1:])**2 + (y_hull[:-1] - y_hull[1:])**2)\n",
    "    dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    spline, u = interpolate.splprep([x_hull, y_hull], \n",
    "                                    u=dist_along, s=0)\n",
    "    interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    interp_x, interp_y = interpolate.splev(interp_d, spline)\n",
    "    # plot shape\n",
    "    plt.fill(interp_x, interp_y, '--', c=pal[i], alpha=0.2)\n",
    "    \n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "\n",
    "Again, following from HW6 [10], [11]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_tsne = TSNE(n_components=3).fit_transform(text_bow_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['bow_tsne']=pcs_tsne[0:len(data_train)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['bow_tsne']=pcs_tsne[len(data_train):len_data].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pc1=np.zeros(len(data_train))\n",
    "bow_pc2=np.zeros(len(data_train))\n",
    "bow_pc3=np.zeros(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_train)):\n",
    "    bow_pc1[i]=data_train['bow_tsne'][i][0]\n",
    "    bow_pc2[i]=data_train['bow_tsne'][i][1]\n",
    "    bow_pc3[i]=data_train['bow_tsne'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['bow_tsne1']=bow_pc1\n",
    "data_train['bow_tsne2']=bow_pc2\n",
    "data_train['bow_tsne3']=bow_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add these columns to the test data\n",
    "\n",
    "bow_pc1=np.zeros(len(data_test))\n",
    "bow_pc2=np.zeros(len(data_test))\n",
    "bow_pc3=np.zeros(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_test)):\n",
    "    bow_pc1[i]=data_test['bow_tsne'][i][0]\n",
    "    bow_pc2[i]=data_test['bow_tsne'][i][1]\n",
    "    bow_pc3[i]=data_test['bow_tsne'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['bow_tsne1']=bow_pc1\n",
    "data_test['bow_tsne2']=bow_pc2\n",
    "data_test['bow_tsne3']=bow_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization_code.py\n",
    "# Directly copy-pasted from the provided file\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['font.family'] = \"serif\"\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "df = data_train\n",
    "pal = sns.color_palette(\"Paired\")[:len(set(data_train['party_class']))]\n",
    "p1 = sns.scatterplot(x=\"bow_tsne1\", y='bow_tsne2', hue='party_class', palette = pal, data=df, s=250, alpha=0.7, legend=False)\n",
    "\n",
    "#For each point, we add a text inside the bubble\n",
    "for line in range(0,df.shape[0]):\n",
    "     p1.text(df.bow_tsne1[line], df.bow_tsne2[line], df.party_class[line], horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "\n",
    "plt.suptitle('Two-Dimensional Map (t-SNE)', fontsize=36)\n",
    "plt.xlabel('Dimension 1', fontsize=24)\n",
    "plt.ylabel('Dimension 2', fontsize=24)\n",
    "\n",
    "\n",
    "for i in df.party_class.unique():\n",
    "    # get the convex hull\n",
    "    points = df[df.party_class == i][['bow_tsne1', 'bow_tsne2']].values\n",
    "    hull = ConvexHull(points)\n",
    "    x_hull = np.append(points[hull.vertices,0],\n",
    "                       points[hull.vertices,0][0])\n",
    "    y_hull = np.append(points[hull.vertices,1],\n",
    "                       points[hull.vertices,1][0])\n",
    "    \n",
    "    # interpolate\n",
    "    dist = np.sqrt((x_hull[:-1] - x_hull[1:])**2 + (y_hull[:-1] - y_hull[1:])**2)\n",
    "    dist_along = np.concatenate(([0], dist.cumsum()))\n",
    "    spline, u = interpolate.splprep([x_hull, y_hull], \n",
    "                                    u=dist_along, s=0)\n",
    "    interp_d = np.linspace(dist_along[0], dist_along[-1], 50)\n",
    "    interp_x, interp_y = interpolate.splev(interp_d, spline)\n",
    "    # plot shape\n",
    "    plt.fill(interp_x, interp_y, '--', c=pal[i], alpha=0.2)\n",
    "    \n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat dimensionality reduction for hashtags vectors\n",
    "\n",
    "Again, following from HW6 [10], [11]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following from [12] to use sklearn's PCA \n",
    "\n",
    "#Standardize before calling PCA\n",
    "scaler=preprocessing.StandardScaler().fit(hash_bow_df)\n",
    "hash_bow_df_scaled=scaler.transform(hash_bow_df)\n",
    "\n",
    "sklearn_pca = PCA(n_components=3)\n",
    "pcs = sklearn_pca.fit_transform(hash_bow_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pcs to the original data\n",
    "data_train['hash_pcs']=pcs[0:len(data_train)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['hash_pcs']=pcs[len(data_train):len_data].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pc1=np.zeros(len(data_train))\n",
    "bow_pc2=np.zeros(len(data_train))\n",
    "bow_pc3=np.zeros(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the principal component returned\n",
    "for i in np.arange(len(data_train)):\n",
    "    bow_pc1[i]=data_train['hash_pcs'][i][0]\n",
    "    bow_pc2[i]=data_train['hash_pcs'][i][1]\n",
    "    bow_pc3[i]=data_train['hash_pcs'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['hash_pc1']=bow_pc1\n",
    "data_train['hash_pc2']=bow_pc2\n",
    "data_train['hash_pc3']=bow_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add these columns to the test data\n",
    "bow_pc1=np.zeros(len(data_test))\n",
    "bow_pc2=np.zeros(len(data_test))\n",
    "bow_pc3=np.zeros(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_test)):\n",
    "    bow_pc1[i]=data_test['hash_pcs'][i][0]\n",
    "    bow_pc2[i]=data_test['hash_pcs'][i][1]\n",
    "    bow_pc3[i]=data_test['hash_pcs'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['hash_pc1']=bow_pc1\n",
    "data_test['hash_pc2']=bow_pc2\n",
    "data_test['hash_pc3']=bow_pc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following from sklearn's guides [14]\n",
    "\n",
    "embedding = SpectralEmbedding(n_components=3)\n",
    "pcs_embedded = embedding.fit_transform(hash_bow_df_scaled) #Use the scaled version of bow_vec column for all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['hash_spec']=pcs_embedded[0:len(data_train)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['hash_spec']=pcs_embedded[len(data_train):len_data].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pc1=np.zeros(len(data_train))\n",
    "bow_pc2=np.zeros(len(data_train))\n",
    "bow_pc3=np.zeros(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_train)):\n",
    "    bow_pc1[i]=data_train['hash_spec'][i][0]\n",
    "    bow_pc2[i]=data_train['hash_spec'][i][1]\n",
    "    bow_pc3[i]=data_train['hash_spec'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['hash_spec1']=bow_pc1\n",
    "data_train['hash_spec2']=bow_pc2\n",
    "data_train['hash_spec3']=bow_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add these columns to the test data\n",
    "\n",
    "bow_pc1=np.zeros(len(data_test))\n",
    "bow_pc2=np.zeros(len(data_test))\n",
    "bow_pc3=np.zeros(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_test)):\n",
    "    bow_pc1[i]=data_test['hash_spec'][i][0]\n",
    "    bow_pc2[i]=data_test['hash_spec'][i][1]\n",
    "    bow_pc3[i]=data_test['hash_spec'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['hash_spec1']=bow_pc1\n",
    "data_test['hash_spec2']=bow_pc2\n",
    "data_test['hash_spec3']=bow_pc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_tsne = TSNE(n_components=3).fit_transform(hash_bow_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['hash_tsne']=pcs_tsne[0:len(data_train)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['hash_tsne']=pcs_tsne[len(data_train):len_data].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pc1=np.zeros(len(data_train))\n",
    "bow_pc2=np.zeros(len(data_train))\n",
    "bow_pc3=np.zeros(len(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_train)):\n",
    "    bow_pc1[i]=data_train['hash_tsne'][i][0]\n",
    "    bow_pc2[i]=data_train['hash_tsne'][i][1]\n",
    "    bow_pc3[i]=data_train['hash_tsne'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['hash_tsne1']=bow_pc1\n",
    "data_train['hash_tsne2']=bow_pc2\n",
    "data_train['hash_tsne3']=bow_pc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add these columns to the test data\n",
    "\n",
    "bow_pc1=np.zeros(len(data_test))\n",
    "bow_pc2=np.zeros(len(data_test))\n",
    "bow_pc3=np.zeros(len(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(len(data_test)):\n",
    "    bow_pc1[i]=data_test['hash_tsne'][i][0]\n",
    "    bow_pc2[i]=data_test['hash_tsne'][i][1]\n",
    "    bow_pc3[i]=data_test['hash_tsne'][i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['hash_tsne1']=bow_pc1\n",
    "data_test['hash_tsne2']=bow_pc2\n",
    "data_test['hash_tsne3']=bow_pc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See how the data sets look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=data_train.loc[:, data_train.columns != 'party_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'party_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'bow_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'bow_spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'bow_tsne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'bow_pcs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'hash_pcs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'hash_spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.loc[:, X_train.columns != 'hash_tsne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=data_train['party_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.to_csv('y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=data_test.loc[:, data_test.columns != 'party_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'party_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'text_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'bow_vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'bow_spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'bow_tsne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'bow_pcs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'hash_pcs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'hash_spec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_test.loc[:, X_test.columns != 'hash_tsne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test #Id column is not considered as a column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id=X_test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id.to_csv('Id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_test['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have ignored the warnings the code following from [15]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python. O’Reilly Media Inc.\n",
    "\n",
    "[2] https://stackoverflow.com/questions/28199524/best-way-to-count-the-number-of-rows-with-missing-values-in-a-pandas-dataframe\n",
    "\n",
    "[3] https://stackoverflow.com/questions/40755680/how-to-reset-index-pandas-dataframe-after-dropna-pandas-dataframe\n",
    "\n",
    "[4] HW4 of Erez\n",
    "\n",
    "[5] HW4 of Kibris\n",
    "\n",
    "[6] Wordnet lemmatization and POS tagging in Python. Stack Overflow. Retrieved February 20, 2022, from https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python \n",
    "\n",
    "[7] Removing emojis from a string in Python. Stack Overflow. Retrieved February 20, 2022, from https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python \n",
    "\n",
    "[8] Caliskan, Cantay. DSCC 465: Introduction to Statistical Machine Learning, Spring 2022, University of Rochester, Rochester NY.\n",
    "\n",
    "[9] https://www.analyticsvidhya.com/blog/2021/08/a-friendly-guide-to-nlp-bag-of-words-with-python-example/\n",
    "\n",
    "[10] HW6 of Erez\n",
    "\n",
    "[11] HW6 of Kibris\n",
    "\n",
    "[12] Galarnyk, M. (2021, November 17). PCA using Python (scikit-learn). Medium. Retrieved April 1, 2022, from https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60 \n",
    "\n",
    "[13] visualization_code.py\n",
    "\n",
    "[14] Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "[15] https://stackoverflow.com/questions/53784971/how-to-disable-convergencewarning-using-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
